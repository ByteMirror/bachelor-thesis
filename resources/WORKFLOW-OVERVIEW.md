# ğŸ¯ Bachelor Thesis - Workflow Overview

## Neue Architektur: Kapitel-basierte Planung

### Struktur
Jedes Kapitel und Unterkapitel hat eine eigene Planungsdatei in `resources/`:

```
resources/
â”œâ”€â”€ 01-einleitung/
â”‚   â”œâ”€â”€ 1.1-hinfuehrung-motivation.md âœ…
â”‚   â”œâ”€â”€ 1.2-problemstellung.md âœ…
â”‚   â”œâ”€â”€ 1.3-zielsetzung-forschungsfrage.md âœ…
â”‚   â””â”€â”€ 1.4-aufbau-der-arbeit.md âœ…
â”œâ”€â”€ 02-theoretische-grundlagen/
â”‚   â”œâ”€â”€ 2.1.1-obsidian-als-markdown-editor.md âœ…
â”‚   â”œâ”€â”€ 2.1.2-electron-framework.md âœ…
â”‚   â”œâ”€â”€ 2.2.1-virtual-tabletop-tools.md âœ…
â”‚   â”œâ”€â”€ 2.2.2-plugin-architekturen.md ğŸ“
â”‚   â”œâ”€â”€ 2.3.1-performance-metriken.md ğŸ“
â”‚   â”œâ”€â”€ 2.3.2-optimierungsstrategien.md ğŸ“
â”‚   â””â”€â”€ 2.4-stand-der-forschung.md ğŸ“
â”œâ”€â”€ 03-konzeption-implementierung/
â”‚   â”œâ”€â”€ 3.1.1-funktionale-anforderungen.md ğŸ“
â”‚   â”œâ”€â”€ 3.1.2-nicht-funktionale-anforderungen.md ğŸ“
â”‚   â”œâ”€â”€ 3.2.1-architekturentwurf.md ğŸ“
â”‚   â”œâ”€â”€ 3.2.2-datenmodell.md ğŸ“
â”‚   â”œâ”€â”€ 3.3.1-rendering-engine-pixi.md ğŸ“ [UPDATED: Framework-Vergleich]
â”‚   â”œâ”€â”€ 3.3.2-grid-system-optimierung.md ğŸ“ [UPDATED: Draw Call Optimization]
â”‚   â”œâ”€â”€ 3.3.3-token-management-performance.md ğŸ“ [UPDATED: Research-basiert]
â”‚   â”œâ”€â”€ 3.3.4-asset-manager-io-optimierung.md ğŸ“ [NEW]
â”‚   â”œâ”€â”€ 3.3-rendering-framework-evaluation.md âœ… [Research-Dokument]
â”‚   â”œâ”€â”€ DOCUMENTATION_INDEX.md âœ… [14 Dokumentationen indexiert]
â”‚   â”œâ”€â”€ 3.4-test-framework.md ğŸ“
â”‚   â””â”€â”€ 3.5-messverfahren.md ğŸ“
â”œâ”€â”€ 04-evaluation-ergebnisse/
â”‚   â”œâ”€â”€ 4.1-durchfuehrung-messungen.md ğŸ“
â”‚   â”œâ”€â”€ 4.2-datenauswertung.md ğŸ“
â”‚   â”œâ”€â”€ 4.3-optimierungsvergleich.md ğŸ“
â”‚   â””â”€â”€ 4.4-diskussion.md ğŸ“
â””â”€â”€ 05-fazit-ausblick/
    â”œâ”€â”€ 5.1-zusammenfassung.md ğŸ“
    â”œâ”€â”€ 5.2-beantwortung-forschungsfrage.md ğŸ“
    â”œâ”€â”€ 5.3-limitationen.md ğŸ“
    â””â”€â”€ 5.4-ausblick.md ğŸ“
```

âœ… = Detaillierte Planung erstellt
ğŸ“ = Platzhalter erstellt (muss noch ausgearbeitet werden)

## Workflow fÃ¼r jedes Unterkapitel

### 1ï¸âƒ£ Planungsphase (in resources/)
**Datei**: `resources/XX-kapitel/Y.Z-unterkapitel.md`

**Inhalt der Planungsdatei:**
- **Kontext und Planung**: Kernthema und Ziel
- **Inhaltliche Struktur**: Geplante AbsÃ¤tze und Argumente
- **Literatur und Quellen**: Zu recherchierende Quellen
- **Code-Beispiele**: Relevante Stellen aus atlas-vtt/
- **Schreibnotizen**: Stil, Fokus, zu vermeiden
- **Verbindungen**: Links zu anderen Kapiteln
- **TODO**: Konkrete Aufgaben beim Schreiben

### 2ï¸âƒ£ Schreibphase (in latex/)
**Basierend auf**: Planungsdokument aus resources/
**Output**: LaTeX-Text in `latex/XX-Kapitel.tex`

**Vorgehen:**
1. Planungsdokument Ã¶ffnen und durcharbeiten
2. Research gemÃ¤ÃŸ Literaturliste durchfÃ¼hren
3. LaTeX-Text verfassen mit `\autocite{}`
4. BibTeX-EintrÃ¤ge parallel in `references.bib` erstellen
5. Code-Beispiele aus `atlas-vtt/` mit Zeilen-Referenzen

### 3ï¸âƒ£ Review-Phase
- Konsistenz zwischen Plan und Text prÃ¼fen
- Alle TODOs abgearbeitet?
- Zitate korrekt?
- Roter Faden vorhanden?
- Deutsche Sprache (auÃŸer Fachbegriffe)?

## Beispiel-Workflow: Kapitel 2.1.1 schreiben

### Schritt 1: Planung reviewen
```bash
# Planungsdokument Ã¶ffnen
cat resources/02-theoretische-grundlagen/2.1.1-obsidian-als-markdown-editor.md
```

### Schritt 2: Research
- Obsidian API Docs durchgehen
- Performance-Metriken sammeln
- Code-Beispiele aus atlas-vtt/ identifizieren

### Schritt 3: LaTeX schreiben
```latex
\subsection{Obsidian als Markdown-Editor}

Obsidian ist ein leistungsfÃ¤higes Knowledge-Management-Tool, das auf
lokalen Markdown-Dateien basiert \autocite{ObsidianDocs2024}. Die
Plugin-Architektur ermÃ¶glicht...
```

### Schritt 4: BibTeX ergÃ¤nzen
```bibtex
@online{ObsidianDocs2024,
    author = {{Obsidian}},
    title = {{Obsidian API Documentation}},
    year = {2024},
    url = {https://docs.obsidian.md},
    urldate = {2024-09-26}
}
```

## Tipps fÃ¼r AI Agents

### Beim Planen (resources/)
- Konkreten Kontext schaffen
- Klare Struktur vorgeben
- Spezifische Literatur identifizieren
- Code-Stellen markieren
- Verbindungen dokumentieren

### Beim Schreiben (latex/)
- IMMER auf Deutsch (auÃŸer Fachbegriffe)
- IMMER `\autocite{}` verwenden
- IMMER BibTeX-Block mitliefern
- Code-Referenzen: `datei.ts:123` Format
- Planungsdokument als Leitfaden

### Beim Reviewen
- Plan vs. Text abgleichen
- Zitate prÃ¼fen
- Konsistenz sicherstellen
- Fachbegriffe einheitlich

---

## ğŸ“ Wissenschaftliche Herangehensweise fÃ¼r Kapitel 3 (Implementierung)

### âš ï¸ WICHTIG: Balance zwischen Programmierung und Wissenschaft

**Kernprinzip**: Kapitel 3 beschreibt die **Implementierung**, aber jede Entscheidung muss **wissenschaftlich begrÃ¼ndet** sein.

#### âœ… Das ist RICHTIG:
```
"Wir haben PIXI.js gewÃ¤hlt, weil Benchmarks zeigten, dass es
2-3x schneller ist als Konva.js (60 FPS vs 23 FPS bei 8k Objekten).
ZusÃ¤tzlich bundelt Obsidian bereits PIXI v7, weshalb wir v8 verwenden
mussten um Konflikte zu vermeiden."
```

#### âŒ Das ist FALSCH:
```
"Wir haben PIXI.js gewÃ¤hlt weil es gut ist."
(Keine BegrÃ¼ndung, keine Daten, nicht wissenschaftlich)
```

---

### Wissenschaftliche AnsÃ¤tze fÃ¼r Implementierungs-Kapitel

#### 1. **Evaluation-basierte Entscheidungen**
**Pattern**: Alternativen vergleichen â†’ Daten sammeln â†’ BegrÃ¼ndete Wahl

**Beispiel**:
- Framework-Vergleich: Konva.js vs Fabric.js vs PIXI.js
- Benchmark-Daten: FPS, Bundle Size, GPU-Nutzung
- Entscheidung: PIXI.js basierend auf Performance-Daten

**Quellen**:
- Externe Benchmarks (z.B. slaylines.io)
- Industry Standards (Foundry VTT nutzt PIXI.js)

---

#### 2. **Research-basierte Implementierung**
**Pattern**: Problem identifizieren â†’ Research â†’ Best Practice finden â†’ Implementieren

**Beispiel**:
- **Problem**: "Wie kÃ¶nnen 100+ Tokens gleichzeitig angezeigt werden ohne Performance-EinbuÃŸen?"
- **Research**: PIXI.js Performance Guide, Foundry VTT Docs, VTT Best Practices
- **Erkenntnisse gefunden**:
  - Culling (`cullable = true`) fÃ¼r off-screen Objekte
  - Batch Rendering (bis zu 16 Textures pro Draw Call)
  - Power-of-Two Textures fÃ¼r GPU-Optimierung
  - Object Pooling zur Vermeidung von Memory-Churn
- **Implementierung**: Diese Techniken anwenden
- **Messung**: In Kapitel 4 validieren

**Quellen**:
- PixiJS Official Performance Guide
- Foundry VTT Media Optimization Guide
- Eigene Code-Dokumentation (z.B. REFACTORING_PLAN.md)

---

#### 3. **Problem-LÃ¶sung-BegrÃ¼ndung**
**Pattern**: Technical Challenge â†’ LÃ¶sungsansÃ¤tze â†’ Trade-offs â†’ BegrÃ¼ndete Entscheidung

**Beispiel**:
- **Problem**: Obsidian-Plugins mÃ¼ssen alles in main.js bÃ¼ndeln (kein Code-Splitting)
- **Trade-off**: Feature-Umfang vs Bundle Size vs Performance
- **LÃ¶sung**: Feature Toggle System fÃ¼r Performance-Evaluation
- **BegrÃ¼ndung**: ErmÃ¶glicht isolierte Messung jedes Features

**Messbar**: Bundle Size pro Feature, Startup Time Impact

---

#### 4. **Refactoring mit messbaren Verbesserungen**
**Pattern**: Problem-Code â†’ Refactoring â†’ Messbare Verbesserung

**Beispiel**:
- **Ausgangslage**: TokenRenderer.ts (3,840 Zeilen) â†’ Wartbarkeits-Problem
- **Refactoring**: Extraktion zu 6 Modulen
- **Ergebnis**:
  - 61% Code-Reduktion (3,840 â†’ 1,490 LoC)
  - 15 Bugs gefunden und gefixt
  - Performance bleibt gleich (wird in Kap. 4 validiert)

**Wissenschaftlich**: Vorher/Nachher-Vergleich mit Metriken

---

### Struktur fÃ¼r Implementierungs-Abschnitte

Jeder Abschnitt in Kapitel 3.3 folgt diesem Pattern:

```markdown
## 3.3.X Feature-Name: Wissenschaftlicher Fokus

### 1. Problem/Kontext
Was musste implementiert werden? Welche Herausforderungen gab es?

### 2. Research/Evaluation
- Welche Alternativen wurden betrachtet?
- Welche Daten/Benchmarks gibt es?
- Was sagen Best Practices/Industry Standards?

### 3. Entscheidung
- Was wurde gewÃ¤hlt und WARUM?
- Mit konkreten Zahlen/Daten begrÃ¼ndet

### 4. Implementierung
- Wie wurde es konkret umgesetzt?
- Code-Beispiele (mit Referenzen)
- Architektur-Entscheidungen

### 5. Messbare Ergebnisse/Trade-offs
- Was wurde erreicht? (mit Zahlen!)
- Welche Trade-offs gab es?
- Verweis auf Kapitel 4 fÃ¼r Validation
```

---

### Dokumentations-Quellen fÃ¼r wissenschaftliche BegrÃ¼ndungen

#### Interne Quellen (atlas-vtt/)
- `tech_stack_and_dependencies.md` - Technologie-Entscheidungen
- `grid-system-modernization-summary.md` - Optimierungs-Beispiel mit Zahlen
- `REFACTORING_PLAN.md` - Refactoring mit messbaren Verbesserungen
- `token-statblock-linking-system.md` - Architektur-Entscheidungen
- `project_purpose_and_architecture.md` - High-Level Design

#### Externe Quellen
- **Benchmarks**: Canvas Engines Comparison (slaylines.io)
- **Best Practices**: PixiJS Performance Guide, Foundry VTT Docs
- **Industry Standards**: Foundry VTT als Reference Implementation

#### Research-Quellen
- WebSearch fÃ¼r aktuelle Best Practices
- Framework-Dokumentationen (PIXI.js, React, Zustand)
- Performance-Optimization Papers/Guides

---

### Was NICHT tun (Anti-Patterns)

#### âŒ Reine Feature-Beschreibung
```
"Der Statblock Builder hat Drag & Drop und unterstÃ¼tzt D&D 5e."
```
**Problem**: Keine wissenschaftliche BegrÃ¼ndung, nur Deskription

#### âŒ Subjektive Aussagen ohne Beleg
```
"PIXI.js ist das beste Framework fÃ¼r VTTs."
```
**Problem**: Keine Daten, keine Vergleiche, nicht wissenschaftlich

#### âŒ Code ohne Kontext
```latex
\begin{lstlisting}
const sprite = new Sprite(texture);
\end{lstlisting}
```
**Problem**: Kein wissenschaftlicher Mehrwert, nur Code-Dokumentation

---

### Was TUN (Best Practices)

#### âœ… Daten-gestÃ¼tzte Entscheidungen
```
"Nach Evaluation von Konva.js (23 FPS) und PIXI.js (60 FPS)
bei 8k Objekten [Quelle: Canvas Engines Comparison] wurde
PIXI.js wegen der 2-3x besseren Performance gewÃ¤hlt."
```

#### âœ… Research-basierte Implementierung
```
"Research von PIXI.js Performance Guides und Foundry VTT
Dokumentation ergab, dass Culling und Batch Rendering die
primÃ¤ren Optimierungstechniken fÃ¼r viele Objekte sind [Quellen].
Diese Techniken wurden implementiert..."
```

#### âœ… Messbare Verbesserungen
```
"Das Refactoring reduzierte die Code-KomplexitÃ¤t von 3,840 auf
1,490 Zeilen (61% Reduktion) und identifizierte 15 Bugs.
Die Performance-Auswirkung wird in Kapitel 4.3 evaluiert."
```

---

### Fokus fÃ¼r Kapitel 3: Die 4 Kern-Features

Basierend auf unserer Diskussion konzentrieren wir uns auf:

1. **3.3.1 Rendering Engine** (PIXI.js)
   - Framework-Vergleich mit Benchmarks
   - PIXI v7 vs v8 Entscheidung

2. **3.3.2 Grid System**
   - Draw Call Optimierung
   - TilingSprite vs Explicit Drawing

3. **3.3.3 Token Creator & Management**
   - Performance bei vielen Objekten (Research-basiert)
   - Modulare Architektur (61% Code-Reduktion)

4. **3.3.4 Asset Manager**
   - I/O-Optimierung (Async vs Sync)
   - Singleton Pattern fÃ¼r Konsistenz

**Ausgelassen** (nach Diskussion):
- âŒ Statblock Builder (zu feature-deskriptiv, schwer wissenschaftlich zu begrÃ¼nden)
- âŒ UI Integration (zu implementierungs-lastig)

---

### Checkliste fÃ¼r jeden Abschnitt

Bevor ein Abschnitt als "fertig" gilt:

- [ ] **Problem klar definiert** - Was musste gelÃ¶st werden?
- [ ] **Alternativen evaluiert** - Mindestens 2 AnsÃ¤tze verglichen
- [ ] **Daten vorhanden** - Benchmarks, Messungen, oder Research-Quellen
- [ ] **Entscheidung begrÃ¼ndet** - WARUM wurde Option X gewÃ¤hlt?
- [ ] **Implementierung beschrieben** - Wie konkret umgesetzt?
- [ ] **Messbare Ergebnisse** - Zahlen! (LoC, FPS, Bundle Size, etc.)
- [ ] **Quellen angegeben** - Mindestens 2-3 Quellen pro Abschnitt
- [ ] **Verbindung zu Kap. 4** - Was wird dort validiert?

---

### Zusammenfassung

**Kapitel 3 = Implementierung + Wissenschaftliche BegrÃ¼ndung**

- **NICHT**: "Hier ist wie Feature X funktioniert" (Code-Dokumentation)
- **SONDERN**: "Wir haben Feature X SO implementiert, weil Research/Evaluation Y zeigte, dass Ansatz Z optimal ist" (Wissenschaftlich begrÃ¼ndete Implementierung)

**Jede Entscheidung braucht**:
1. Research/Evaluation (mit Quellen)
2. Daten/Benchmarks (mit Zahlen)
3. BegrÃ¼ndung (WARUM nicht anders?)
4. Messbare Ergebnisse (Validation in Kap. 4)

## Fortschritts-Tracking

### Aktueller Status
- **Kapitel 1**: VollstÃ¤ndig geplant âœ…
- **Kapitel 2**: Teilweise geplant (3/7) ğŸŸ¡
- **Kapitel 3**: Platzhalter erstellt ğŸ”´
- **Kapitel 4**: Platzhalter erstellt ğŸ”´
- **Kapitel 5**: Platzhalter erstellt ğŸ”´

### NÃ¤chste Schritte
1. [ ] Restliche Planungen fÃ¼r Kapitel 2 ausarbeiten
2. [ ] Atlas VTT Code-Analyse fÃ¼r Kapitel 3
3. [ ] Performance-Test Framework konzipieren
4. [ ] Erste LaTeX-Texte basierend auf Planungen schreiben

## Commands fÃ¼r Agents

### Planung erstellen
```
"Erstelle eine detaillierte Planung fÃ¼r Kapitel X.Y basierend auf dem Template und dem Kontext aus atlas-vtt/"
```

### Text schreiben
```
"Schreibe den LaTeX-Text fÃ¼r Kapitel X.Y basierend auf der Planung in resources/XX-kapitel/X.Y-titel.md. Liefere LaTeX-Code und BibTeX-EintrÃ¤ge."
```

### Code analysieren
```
"Analysiere atlas-vtt/ fÃ¼r relevante Code-Beispiele zu [Thema] und erstelle Referenzen im Format datei.ts:zeile"
```

---

**Dieser Workflow stellt sicher, dass:**
- Jedes Kapitel strukturiert geplant wird
- AI Agents klaren Kontext haben
- Konsistenz gewÃ¤hrleistet ist
- Wissenschaftliche Standards eingehalten werden
- Der Fortschritt nachvollziehbar ist