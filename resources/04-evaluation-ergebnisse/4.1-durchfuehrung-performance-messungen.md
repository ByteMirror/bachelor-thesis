# Planungsdokument: Kapitel 4.1 - Durchführung der Performance-Messungen

**Datum**: 2025-01-16
**Status**: Planning Phase
**Zielseitenzahl**: 3 Seiten (von 14 Seiten für Kapitel 4)

---

## Kontext und Kernbotschaft

### Position im Gesamtwerk
- **Kapitel 4** ist das Evaluationskapitel der Thesis
- **4.1** ist der erste Abschnitt: Beschreibung der **Methodik** der Performance-Messungen
- Folgt auf Kapitel 3 (Konzeption und Implementierung)
- Bereitet auf 4.2 (Auswertung) vor

### Kernbotschaft
Wissenschaftlich fundierte, reproduzierbare Performance-Messmethodik für Electron-basierte Plugins, die **state-of-the-art Benchmarking-Prinzipien** folgt und die **Forschungsfrage** beantwortet:

> Welche Herausforderungen ergeben sich bei der Umsetzung eines VTT-Plugins in Obsidian.md, und wie beeinflussen diese Performance, Wartbarkeit und Entwicklungsaufwand?

---

## Inhaltliche Struktur

### 4.1.1 Testumgebung und -bedingungen (ca. 1 Seite)

**Ziel**: Transparenz und Reproduzierbarkeit sicherstellen

**Inhalt**:
1. **Hardware-Spezifikationen**
   - Exakte CPU, RAM, GPU, Storage-Angaben
   - Begründung der Hardware-Auswahl (repräsentativ für Zielgruppe)

2. **Software-Versionen**
   - Obsidian Version
   - Electron Version
   - Node.js, PIXI.js v8
   - Atlas VTT Plugin Version

3. **Kontrollierte Testbedingungen**
   - Keine Hintergrundprozesse
   - Standardisierte Systemlast
   - Cache-Management

4. **Testdaten und Szenarien**
   - Leere Map (Baseline)
   - Standard-Session (20-30 Token)
   - Stress-Test (100+ Token)
   - Interaktions-Test (Drag & Drop)

**Literatur**:
- NIH Essential Guidelines for Computational Method Benchmarking → Transparenz
- Thangadurai et al. 2024 → Multi-Factor Experimental Design

---

### 4.1.2 Messmethodik (ca. 1.5-2 Seiten)

**Ziel**: Wissenschaftliche Rigorosität demonstrieren

**Inhalt**:

1. **Feature-Isolierung**
   - Die 5 untersuchten Features:
     1. PIXI.js Rendering Engine
     2. Statblock Builder/System
     3. Asset Manager
     4. Token Creator/Management
     5. Grid System
   - Feature-Toggle-Mechanismus
   - Sicherstellung vollständiger Isolation

2. **Metriken-Framework**
   - **User-Perceived Metrics** (Primär):
     - Click Latency (ms)
     - Keypress Latency (ms)
     - Time to Interactive (ms)
   - **Technical Metrics** (Sekundär):
     - Startup Time (ms)
     - FPS
     - Heap Size (MB)
     - Bundle Size (KB)

3. **Mess-Tools und -Verfahren**
   - Chrome DevTools Performance Panel
   - PerformanceObserver API
   - hyperfine (CLI Benchmarking)
   - Custom Telemetry Service

4. **Statistische Methoden**
   - Anzahl der Testläufe (n ≥ 10)
   - Warmup-Runs (n = 3)
   - Normalverteilung prüfen (Shapiro-Wilk Test)
   - ANOVA für Multi-Group Vergleiche
   - Post-hoc Tests (Tukey HSD)
   - Effektgrößen (Cohen's d)
   - 95% Konfidenzintervalle

5. **Validitätssicherung**
   - **Interne Validität**: Kontrolle von Störfaktoren
   - **Externe Validität**: Repräsentative Szenarien
   - **Konstruktvalidität**: Richtige Metriken

**Literatur**:
- Weber et al. 2019 (NIH): Essential Guidelines for Computational Method Benchmarking
- Thangadurai et al. 2024: Electron App Performance Study
- Nolan Lawson 2021: JavaScript Performance Beyond Bundle Size
- Montgomery 2017: Design and Analysis of Experiments

---

## Verbindungen zu anderen Kapiteln

### Rückbezüge
- **Kapitel 2.3**: Performance-Analyse und Benchmarking (theoretische Grundlagen)
- **Kapitel 3.4**: Implementierung der Testumgebung (technische Umsetzung)
- **Kapitel 3.5**: Dokumentation der Messverfahren (Code-Level Details)

### Vorausblick
- **Kapitel 4.2**: Auswertung und Interpretation (verwendet diese Methodik)
- **Kapitel 4.3**: Vergleich verschiedener Optimierungsstrategien (verwendet diese Daten)
- **Kapitel 4.4**: Diskussion der Ergebnisse (wertet diese Methodik aus)

---

## Literatur und Quellen

### Primärliteratur (Peer-Reviewed)

1. **Weber et al. 2019** - "Essential guidelines for computational method benchmarking"
   **PMC**: https://pmc.ncbi.nlm.nih.gov/articles/PMC6584985/
   **Zitat-Kontext**:
   > "evaluation of methods will rely on one or more quantitative performance metrics"
   > "the choice of metric depends on the type of method and data"
   > "without a thorough discussion of limitations, a benchmark runs the risk of misleading readers"

2. **Thangadurai et al. 2024** - "Electron vs. Web: A Comparative Analysis of Energy and Performance in Communication Apps"
   **Springer**: https://link.springer.com/chapter/10.1007/978-3-031-70245-7_13
   **Zitat-Kontext**:
   > "exhaustive multi-factor and multi-treatment experimental design"
   > "scenarios accounting for platform types, modes of interaction, and interaction durations"
   > "Speedometer 2.0 benchmark showed negligible completion time increase"

3. **Montgomery 2017** - "Design and Analysis of Experiments" (9th Edition)
   **Wiley**
   **Zitat-Kontext**: Statistical methods for experimental design, ANOVA, effect sizes

4. **Basili, Selby & Hutchens 1986** - "Experimentation in Software Engineering"
   **IEEE**: DOI: 10.1109/TSE.1986.6312975
   **Zitat-Kontext**: Experimental design principles for software performance evaluation

### Technische Dokumentation

5. **Electron Performance Guide 2024**
   **URL**: https://www.electronjs.org/docs/latest/tutorial/performance
   **Zitat-Kontext**:
   > "profile the running code, find the most resource-hungry piece"
   > "this practice is by far the most reliable strategy to improve performance"

6. **Chrome DevTools Performance Documentation 2024**
   **URL**: https://developer.chrome.com/docs/devtools/performance/
   **Zitat-Kontext**: Measurement tools and profiling techniques

7. **Nolan Lawson 2021** - "JavaScript performance beyond bundle size"
   **URL**: https://nolanlawson.com/2021/02/23/javascript-performance-beyond-bundle-size/
   **Zitat-Kontext**:
   > "most valuable metrics are those closest to measuring the user's perceived performance"
   > "click latency, keypress latency, scroll latency, and time to feature paint"

### Tool-Dokumentation

8. **hyperfine** - Command-line benchmarking tool
   **GitHub**: https://github.com/sharkdp/hyperfine
   **Zitat-Kontext**: Statistical rigor, warmup runs, multiple iterations

---

## TODOs für Schreibphase

### Vor dem Schreiben
- [ ] Alle BibTeX-Einträge in references.bib eintragen
- [ ] Hardware-Spezifikationen dokumentieren
- [ ] Genaue Software-Versionen notieren
- [ ] Feature-Liste finalisieren

### Während des Schreibens
- [ ] Jeden Absatz mit Quelle belegen (\autocite{})
- [ ] Metriken-Tabelle erstellen (LaTeX table)
- [ ] Experimentelles Design als Diagramm
- [ ] Statistische Methoden präzise beschreiben

### Nach dem Schreiben
- [ ] Alle Zitate überprüfen
- [ ] Konsistente Terminologie
- [ ] Wissenschaftlicher Ton
- [ ] Seitenzahl ~3 Seiten prüfen

---

## Wichtige Zitatstellen mit Kontext

### NIH Guidelines (Weber et al. 2019)
**Page**: Introduction & Discussion sections
**Relevante Stellen**:
- "evaluation of methods will rely on one or more quantitative performance metrics" → Metriken-Auswahl
- "the choice of metric depends on the type of method and data" → Begründung unserer Metriken
- "without a thorough discussion of limitations, a benchmark runs the risk of misleading readers" → Validitätssicherung
- "subjectivity in the choice of datasets or evaluation metrics could bias the results" → Transparenz

**Verwendung in Thesis**:
→ Abschnitt 4.1.2 (Messmethodik), bei Metriken-Auswahl

---

### Thangadurai et al. 2024
**Page**: Methodology section
**Relevante Stellen**:
- "exhaustive multi-factor and multi-treatment experimental design" → Unser Feature-Isolation Ansatz
- "scenarios accounting for platform types, modes of interaction, and interaction durations" → Unsere 4 Test-Szenarien
- "Speedometer 2.0 benchmark" → Realistische Workload-Simulation

**Verwendung in Thesis**:
→ Abschnitt 4.1.1 (Testumgebung), bei Szenario-Beschreibung
→ Abschnitt 4.1.2 (Messmethodik), bei experimentellem Design

---

### Nolan Lawson 2021
**Section**: "What metrics should we measure?"
**Relevante Stellen**:
- "most valuable metrics are those closest to measuring the user's perceived performance" → User-Perceived Metrics
- "Web Vitals run short on measuring what's often slow in Electron apps" → Warum wir custom metrics brauchen
- "click latency, keypress latency, scroll latency" → Unsere primären Metriken

**Verwendung in Thesis**:
→ Abschnitt 4.1.2 (Messmethodik), bei Metriken-Framework

---

### Electron Performance Guide 2024
**Section**: "Profile Performance Bottlenecks"
**Relevante Stellen**:
- "profile the running code, find the most resource-hungry piece of it" → Unser Tool-Auswahl (Chrome DevTools)
- "this practice is by far the most reliable strategy" → Begründung für Profiling-Ansatz

**Verwendung in Thesis**:
→ Abschnitt 4.1.2 (Messmethodik), bei Mess-Tools

---

## Wissenschaftlicher Schreibstil - Checkliste

- ✅ Deutsch für Fließtext, Englisch für Fachbegriffe
- ✅ Immer \autocite{} für Quellen
- ✅ Wissenschaftlich präzise, aber verständlich
- ✅ Keine Ich-Form, passiv formulieren
- ✅ Begründungen für jede methodische Entscheidung
- ✅ Transparenz über Limitationen
- ✅ Roter Faden: Von Testumgebung → Messmethodik → Validität

---

**Ende Planungsdokument**
